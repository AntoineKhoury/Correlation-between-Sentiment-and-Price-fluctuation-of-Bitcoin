{"cells":[{"cell_type":"markdown","source":["# Description\n","\n","> This notebook is responsible for pre-training BerTweet on a new data set comprised only on tweets about cryptocurrencies and especially about Bitcoin.\n"," The dataset is composed of 16912 tweets about cryptocurrencies.\n"," For our model, we first pre-processed the tweets as the authors of Bertweet did (normalize each tweet) then we replaced some words related to a topic by generic keywords. \n","\n","\n","> For instance, \"Bitcoin\" was replaced by \"bitcn\" surrounded by < and >.\n","After that preprocessing was done, the Bertweet model was then further pre-trained on the dataset using the Masked Language Modelling (MLM) procedure with tokens replaced 15% of the time by eithert the mask or another token.\n"],"metadata":{"id":"cvia_XGy8TgR"}},{"cell_type":"markdown","source":["# Necessary imports"],"metadata":{"id":"jiRFLImD-kOQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGJBI-Vd_cXz"},"outputs":[],"source":["!pip install pandas\n","\n","!pip install torch\n","\n","!pip install transformers\n","\n","!pip install emoji\n","\n","!pip install numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SUXS0yw-Oc0"},"outputs":[],"source":["# Imports\n","import pandas as pd\n","import torch\n","import numpy as np\n","from transformers import AutoModelForMaskedLM,AutoTokenizer,AutoModelForSequenceClassification\n","import emoji"]},{"cell_type":"markdown","source":["# Functions used in preprocessing"],"metadata":{"id":"KNRaI4An_Koz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"brpssSux-cEh"},"outputs":[],"source":["# This is the dictionary of strings that should be replaced into a token useful for us\n","dict_tok_subs = {'<bitcn>':['bitc','Bitc','BITC','Btc','btc','BTC'], '<coin>':['#coin','coin','Coin','COIN'], '<address>':['Address', 'address', 'ADDRESS'], '<block>':['Blockchain', 'blockchain', 'BLOCKCHAIN', 'Block Chain', 'block chain', 'BLOCK CHAIN'], '<confirmation>': ['Confirmation', 'confirmation', 'CONFIRMATION'], '<cryptography>':['Cryptography', 'cryptography', 'CRYPTOGRAPHY'], '<doublespend>': ['doublespend', 'Doublespend', 'DOUBLESPEND', 'double spend', 'Double Spend', 'DOUBLE SPEND'],\n","                    '<hashrate>': ['Hash Rate', 'HASH RATE', 'hash rate'], '<mining>': ['MINING', 'mining', 'Mining'], '<p2p>' : ['p2p', 'P2P', 'peer-to-peer', 'Peer-to-peer'], '<privatekey>': ['Private Key', 'private key', 'PRIVATE KEY','privatekey', 'PRIVATEKEY', 'Privatekey'],\n","                    '<signature>':['SIGNATURE', 'Signature', 'signature'] , '<wallet>':['Wallet', 'WALLET', 'wallet'],\n","                 '<price>': ['price', 'PRICE', 'Price'], '<buy>': ['buy', 'BUY', 'Buy'], '<pump>': ['pump', 'PUMP', 'Pump'],\n","                 '<profit>': ['PROFIT', 'profit', 'Profit'], '<volume>': ['volume', 'Volume', 'VOLUME'],\n","                 '<etf>': ['ETF', 'etf', 'Etf'], '<bull>': ['bull', 'Bull', 'BULL'], '<sell>': ['sell', 'SELL', 'Sell'],\n","                 '<top>': ['top', 'TOP', 'Top'], '<win>': ['win', 'WIN', 'Win'], '<moon>': ['moon', 'MOON', 'Moon'],\n","                 '<signal>': ['signal', 'SIGNAL', 'Signal'], '<long>': ['long', 'LONG', 'Long'], '<chart>': ['CHART', 'chart', 'Chart'],\n","                 '<alts>': ['alts', 'ALTS', 'Alts'], '<hodl>': ['hodl', 'HODL', 'Hodl'], '<support>': ['support', 'SUPPORT', 'Support'],\n","                 '<short>': ['short', 'Short', 'SHORT'], '<drop>': ['drop', 'DROP', 'Drop'], '<project>': ['project', 'PROJECT', 'Project'],\n","                 '<bullish>': ['bulllish', 'Bullish', 'BULLISH'], '<fall>': ['fall', 'Fall', 'FALL'], '<dump>': ['dump', 'DUMP', 'Dump'],\n","                 '<bear>': ['bear', 'Bear', 'BEAR'], '<resistance>': ['resistance', 'RESISTANCE', 'Resistance'], '<opportunity>': ['opportunity', 'OPPORTUNITY', 'Opportunity'],\n","                 '<stop-loss>': ['stop-loss', 'stop loss', 'STOP-LOSS'], '<volume>': ['Volume', 'VOLUME', 'volume'],\n","                 '<chain>': ['chain', 'Chain', 'CHAIN'], '<hold>': ['hold', 'Hold', 'HOLD'], '<future>': ['future', 'FUTURE', 'Future'],\n","                 '<value>': ['value', 'Value', 'VALUE'], '<trader>': ['trader', 'Trader', 'TRADER'], '<nft>': ['nft', 'NFT', 'Nft'],\n","                 '<launch>': ['launch', 'Launch', 'LAUNCH'], '<fiat>': ['fiat', 'Fiat', 'FIAT'], '<liquid>': ['liquid', 'Liquid', 'LIQUID'],\n","                 '<scam>': ['scam', 'Scam', 'SCAM']}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EP4TrTLI-ekj"},"outputs":[],"source":["# Get the list of tokens that should be added\n","list_tokens = list(dict_tok_subs.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmI4IZUY-hpo"},"outputs":[],"source":["def replace_by_token(lst_of_tweets, tokens_dictionary):\n","  \"\"\"_summary_\n","    In this function, words that are significant for bitcoin tweets are replaced\n","    by appropriate words.\n","\n","    Parameters\n","    ----------\n","    input : list, dict\n","      The function takes as list of tweets\n","      and a dictionary with signifcant substrings of words which are assigned \n","      to an appropriate token.\n","      The order in which the keys in the the dictionary are placed matters,\n","      as words that could be assigned to two different tokens will be replaced by \n","      the token that shows up first in the dictionary.\n","\n","    Returns\n","    -------\n","    output : list\n","      It returns a list of tweets in which the  significant words \n","      of each tweet are replaced.\n","  \"\"\"\n","  list_tweets = []\n","  #loop thourgh every tweet\n","  for text in lst_of_tweets:\n","    splits = text.split(\" \")\n","\n","    for split in range(len(splits)):\n","        #go to every word\n","        for key in tokens_dictionary:\n","            #loop through substrings that are associated with each replacement word\n","            for possible_string in tokens_dictionary[key]:\n","                if possible_string in splits[split]:\n","                    splits[split]=key\n","\n","    text_tok = ' '.join(splits)\n","    list_tweets.append(text_tok)\n","\n","  return list_tweets\n"]},{"cell_type":"markdown","source":["# Loading the model serving as a basis"],"metadata":{"id":"bGEPQpXd_QpX"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12921,"status":"ok","timestamp":1648422728572,"user":{"displayName":"Antoine K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06899832850705609817"},"user_tz":-60},"id":"9V7ODl1_-o-H","outputId":"e443d855-a5cb-4c9a-8a65-0ce0ec7d4c76"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["Embedding(64051, 768)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Load the bertweet with additional tokens\n","bertweet = AutoModelForMaskedLM.from_pretrained(\"vinai/bertweet-base\")\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",additional_special_tokens =list_tokens)\n","bertweet.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XULTmGGm-vBv"},"outputs":[],"source":["# Tests to see if the tokens have been added to the tokenizer and model\n","\"<bitcn>\" in tokenizer.get_vocab()\n","#\n","special_token_id = tokenizer.convert_tokens_to_ids([\"<bitcn>\"])\n","print(special_token_id)\n","#%% Check it is adding the tokens correctly\n","query = \"Hey this is a <bitcn> token\"\n","data = [\n","    [\"Pos\"],\n","    [\"1\"],\n","    [\"2\"],\n","]\n","table = pd.DataFrame.from_records(data[1:], columns=data[0])\n","p_output = tokenizer.encode(query)\n","print(p_output)\n","\n","\n","#%%\n","print(tokenizer.encode(\"<bitcn>\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23485,"status":"ok","timestamp":1648473424844,"user":{"displayName":"Antoine K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06899832850705609817"},"user_tz":-60},"id":"9eacb5Vw-v4O","outputId":"bff07eee-3724-4fb8-fd80-30e471e02a69"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n","16912\n"]}],"source":["# Load the dataset used in pre-training\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","\n","dataset = pd.read_csv(\"/content/gdrive/My Drive/NLP Group Project (2022)/tweet_datasets/btc_tweet_20000_without_Sha_label - btc_tweet_20000.csv\")\n","data_used_in_pretraining = dataset.iloc[1000:]\n","print(len(data_used_in_pretraining))\n","tweets = data_used_in_pretraining[\"text\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJzi5iCZ-0XZ"},"outputs":[],"source":["# Normalize all the tweets using the bertweet procedure\n","normalized_tweets = []\n","for tweet in range(len(tweets)):\n","    normalized_tweets.append(tokenizer.normalizeTweet(str(tweets.iloc[tweet])))\n","    if tweet%100 == 0:\n","        print(\"Did 100 more\")\n","        print(tweet)\n","tweets[\"normalized\"] = normalized_tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzWeUnrR-3Zz"},"outputs":[],"source":["# Pre-process the tweets using the function that replaces words by generic tokens\n","tweets_tokenized = replace_by_token(tweets[\"normalized\"],dict_tok_subs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbDegrUb---w"},"outputs":[],"source":["# Save the normalized and preprocessed tweets in a csv in case we re-use them\n","tweets_tokenized_df = pd.DataFrame(tweets_tokenized)\n","tweets_tokenized_df.to_csv(\"/content/gdrive/My Drive/NLP Group Project (2022)/pre-training_and_fine-tuning_bertweet/tweets_normalized_df.txt\", index = False,header = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZZv9Nfj_Cdx"},"outputs":[],"source":["# Load data set in hugging face format and train the model on it\n","from transformers import LineByLineTextDataset\n","from transformers import Trainer, TrainingArguments\n","from transformers import DataCollatorForLanguageModeling\n","\n","dataset = LineByLineTextDataset(\n","    tokenizer=tokenizer,\n","    file_path=\"tweets_normalized_df.txt\",\n","    block_size=64,\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"/content/gdrive/My Drive/NLP Group Project (2022)/bertweet-retrained\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=48,\n","    save_steps=500,\n","    save_total_limit=2,\n","    seed=1\n",")\n","\n","trainer = Trainer(\n","    model=bertweet,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")\n","\n","trainer.train()\n","\n","trainer.save_model(\"/content/gdrive/My Drive/NLP Group Project (2022)/bertweet-retrained\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8285,"status":"ok","timestamp":1648473696501,"user":{"displayName":"Antoine K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06899832850705609817"},"user_tz":-60},"id":"hofvLTjgrobO","outputId":"dc573b1c-817d-4f19-b7bf-bcd282c28eaa"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Since the model will stop training and save at every 500 optimizatin steps, it needs to be restarted the following way\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import LineByLineTextDataset\n","from transformers import Trainer, TrainingArguments\n","\n","bertweet_retrained = AutoModelForMaskedLM.from_pretrained(\"/content/gdrive/My Drive/NLP Group Project (2022)/bertweet-retrained/checkpoint-500\")\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",additional_special_tokens =list_tokens)\n","\n","dataset = LineByLineTextDataset(\n","    tokenizer=tokenizer,\n","    file_path=\"/content/gdrive/My Drive/NLP Group Project (2022)/tweets_normalized_df.txt\",\n","    block_size=64,\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"/content/gdrive/My Drive/NLP Group Project (2022)/bertweet-retrained\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=48,\n","    save_steps=500,\n","    save_total_limit=2,\n","    seed=1\n",")\n","\n","trainer = Trainer(\n","    model=bertweet_retrained,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")\n","\n","trainer.train(\"/content/gdrive/My Drive/NLP Group Project (2022)/bertweet-retrained/checkpoint-500\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LwEYLRFBnrQU"},"outputs":[],"source":["# When it is done training, you can save the model running the following command\n","trainer.save_model(\"/content/gdrive/My Drive/NLP Group Project (2022)/bertweet-retrained\")"]},{"cell_type":"markdown","source":["Testing after pre-training\n","\n","\n","> To see if the model pre-trained correctly, run the following cell and see by which word our model would replace the missing word (named < mask > in our string)\n","\n","\n","\n"],"metadata":{"id":"QrFSXYI0A9dj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yo4Q9zTL_DM-"},"outputs":[],"source":["from transformers import pipeline\n","\n","fill_mask = pipeline(\n","    \"fill-mask\",\n","    model=\"/content/gdrive/My Drive/NLP Group Project (2022)/bertweet-retrained\",\n","    tokenizer=tokenizer\n",")\n","fill_mask(\"The price of <mask> !\")\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Pre-training-Bertweet.ipynb","provenance":[],"authorship_tag":"ABX9TyOvQU6K6pRg5nZC2GFa8+Lz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}